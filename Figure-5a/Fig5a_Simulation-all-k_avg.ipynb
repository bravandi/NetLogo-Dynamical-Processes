{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383900f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib as mpl\n",
    "import datetime\n",
    "import math\n",
    "from scipy.stats import norm, kurtosis, mode\n",
    "cmap = plt.get_cmap('tab10')\n",
    "from scipy.stats import describe\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d24bd180-dc08-4072-afb5-61ea3d0c9be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a-ER-k_avg5instance-85.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg5instance-86.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg5instance-40.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg5instance-18.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg5instance-39.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg5instance-77.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg4.5instance-18.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg5instance-4.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg5instance-94.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg5instance-99.pickle_endtime.npy --> 100\n",
      "5a-ER-k_avg5instance-62.pickle_endtime.npy --> 100\n"
     ]
    }
   ],
   "source": [
    "# np.save(path_save, np.array(end_time_all_ER))\n",
    "for f in os.listdir('res'):\n",
    "    if f.endswith('.DS_Store'):\n",
    "        continue\n",
    "\n",
    "    len_array = len(np.load('res/' + f))\n",
    "    \n",
    "    print('{} --> {}'.format(\n",
    "        f,\n",
    "        len_array\n",
    "    ))\n",
    "\n",
    "    if len_array != 100:\n",
    "        # os.remove('res/' + f)\n",
    "        pass\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c921da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_simulation(p_out,G_input, N_walkers,initial_node_with_walkers,run_time):\n",
    "    end_time_list = []\n",
    "    r_square_re = []\n",
    "    for iter_num in range(run_time):\n",
    "        #G = G_ba.copy()\n",
    "        G = G_input.copy()\n",
    "        N_nodes = len(G.nodes())\n",
    "\n",
    "\n",
    "        ### Asign the walker\n",
    "        nx.set_node_attributes(G, [0],'history')\n",
    "        initial_walkers = random.sample(range(N_nodes),initial_node_with_walkers)\n",
    "        for i in initial_walkers:\n",
    "            G.nodes[i]['history'] = [N_walkers / initial_node_with_walkers]\n",
    "        \n",
    "        degree_of_initial = [G.degree(k) for k in initial_walkers]\n",
    "\n",
    "        # the x (expected final distribution of walkers) of the linear regression\n",
    "        walker_per_degree = N_walkers/sum([j for (i,j) in G.degree])\n",
    "        degree_list = np.array([j*walker_per_degree for (i,j) in G.degree])\n",
    "        degree_list = degree_list.reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        # Run the simulation model until the R-square over 0.99\n",
    "        linear_score = [0]\n",
    "        end_time = 0\n",
    "        while linear_score[-1] <0.99:\n",
    "            end_time += 1\n",
    "            for nodes in G.nodes:\n",
    "                tem_out_list = [n for n in G.neighbors(nodes)]\n",
    "                tem_in_node = 0\n",
    "                for neigh_node in tem_out_list:\n",
    "                    tem_in_node += G.nodes[neigh_node]['history'][end_time-1] * p_out / len([n for n in G.neighbors(neigh_node)])\n",
    "                G.nodes[nodes]['history'] = G.nodes[nodes]['history']+[tem_in_node + G.nodes[nodes]['history'][end_time-1] * (1-p_out)]\n",
    "            ## Run the linear regression\n",
    "            check_list = []\n",
    "            for nodes in G.nodes:\n",
    "                check_list.append(G.nodes[nodes]['history'][-1])\n",
    "            reg = LinearRegression().fit(degree_list, check_list)\n",
    "            linear_score.append(round(reg.score(degree_list, check_list),4))\n",
    "        # record the end time of each simulation    \n",
    "        end_time_list.append(end_time)\n",
    "        \n",
    "    return end_time_list"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7cf92b2-1fc2-45b4-a847-ae4a928a48c5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "k_avg_list = [\n",
    "    # 1,1.5,2,2.5,3,3.5,\n",
    "    4,4.5,5,5.5,6,6.5,7,7.5,8,8.5,9,9.5,10\n",
    "]\n",
    "p_out = 0.4\n",
    "N_walkers = 10000\n",
    "initial_node_with_walkers = 4\n",
    "run_time = 100\n",
    "\n",
    "for k_avg in tqdm(k_avg_list):\n",
    "\n",
    "    end_time_all_ER = []\n",
    "\n",
    "    path_save = 'data/5a-end_time_all_ER_k_avg-' + str(k_avg) + '.npy'\n",
    "    print('path_save (check): ', path_save)\n",
    "\n",
    "    if os.path.exists(path_save):\n",
    "        print('EXISTS')\n",
    "        continue\n",
    "    \n",
    "    for _ in tqdm(range(1)): ## 100 graph instances\n",
    "        \n",
    "        G_ER_total = nx.read_gpickle('data/5a-ER-k_avg' + str(k_avg) + 'instance-' + str(_) + '.pickle')\n",
    "\n",
    "        Gcc = sorted(nx.connected_components(G_ER_total), key=len, reverse=True)\n",
    "        G_ER = G_ER_total.subgraph(Gcc[0])\n",
    "        if G_ER_total.number_of_nodes() != G_ER.number_of_nodes():\n",
    "            print('N(G_ER_total): {} | N(G_ER): {}'.format(G_ER_total.number_of_nodes(), G_ER.number_of_nodes()))\n",
    "        #print(G_ER.number_of_nodes())\n",
    "\n",
    "        end_time_list_ER = one_simulation(p_out,G_ER, N_walkers, initial_node_with_walkers, run_time)\n",
    "    \n",
    "        end_time_all_ER.append(end_time_list_ER)\n",
    "        # np.save(path_save, np.array(end_time_all_ER))\n",
    "        pass\n",
    "        \n",
    "    np.save(path_save, np.array(end_time_all_ER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec84220f-0d65-4fe9-9e15-3d27a3865257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 700.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_g_path = []\n",
    "\n",
    "for k_avg in tqdm(k_avg_list):\n",
    "  \n",
    "    for _ in range(100): ## 100 graph instances\n",
    "\n",
    "        path_g = 'data/5a-ER-k_avg' + str(k_avg) + 'instance-' + str(_) + '.pickle'\n",
    "        if os.path.exists(path_g) is False:\n",
    "            raise Exception('path_g does not exists: {}'.format(path_g))\n",
    "            pass\n",
    "\n",
    "        all_g_path.append(path_g)\n",
    "        \n",
    "        pass\n",
    "    pass\n",
    "\n",
    "len(all_g_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXISTS] res/5a-ER-k_avg4instance-0.pickle_endtime.npy\n",
      "[EXISTS] res/5a-ER-k_avg4instance-1.pickle_endtime.npy\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-4.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-2.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-3.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-5.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-6.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-8.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-7.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-10.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-9.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-11.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-12.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-13.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-14.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-15.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-16.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-17.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-18.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-19.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-22.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-21.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-20.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-23.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-24.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-25.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-26.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-27.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-28.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-29.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-30.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-32.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-31.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-34.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-33.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-35.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-37.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-36.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-38.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-39.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-40.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-41.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-42.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-43.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-44.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-45.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-46.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-48.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-47.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-49.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-50.pickle\n",
      "[WORKING ON GRAPH] data/5a-ER-k_avg4instance-51.pickle\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import datetime\n",
    "\n",
    "\n",
    "k_avg_list = [\n",
    "    # 1,1.5,2,2.5,3,3.5,\n",
    "    4,4.5,5,5.5,6,6.5,7,7.5,8,8.5,9,9.5,10\n",
    "]\n",
    "p_out = 0.4\n",
    "N_walkers = 10000\n",
    "initial_node_with_walkers = 4\n",
    "run_time = 100\n",
    "\n",
    "def task(path):\n",
    "    path_end_time = 'res/' + path.split('/')[1] + '_endtime.npy'\n",
    "\n",
    "    if os.path.exists(path_end_time):\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        time_str = current_datetime.strftime(\"%H:%M:%S %m/%d/%Y\")\n",
    "        print('[EXISTS | {}] {}'.format(time_str, path_end_time))\n",
    "        return\n",
    "\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    time_str = current_datetime.strftime(\"%H:%M:%S %m/%d/%Y\")\n",
    "    print('[WORKING ON GRAPH | {}] {}'.format(time_str, path))\n",
    "    \n",
    "    G_ER_total = nx.read_gpickle(path)\n",
    "\n",
    "    Gcc = sorted(nx.connected_components(G_ER_total), key=len, reverse=True)\n",
    "    G_ER = G_ER_total.subgraph(Gcc[0])\n",
    "    \n",
    "    if G_ER_total.number_of_nodes() != G_ER.number_of_nodes():\n",
    "        print('N(G_ER_total): {} | N(G_ER): {}'.format(G_ER_total.number_of_nodes(), G_ER.number_of_nodes()))\n",
    "    #print(G_ER.number_of_nodes())\n",
    "\n",
    "    end_time_list_ER = one_simulation(p_out, G_ER, N_walkers, initial_node_with_walkers, run_time)\n",
    "    \n",
    "    np.save(path_end_time, end_time_list_ER)\n",
    "\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    time_str = current_datetime.strftime(\"%H:%M:%S %m/%d/%Y\")\n",
    "    print('[DONE! SAVED | {}] {}'.format(time_str, path_end_time))\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Set the maximum number of workers\n",
    "max_workers = 500\n",
    "\n",
    "# Create a thread pool executor with the specified maximum workers\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit tasks to the executor\n",
    "    for p in all_g_path:\n",
    "        \n",
    "        executor.submit(task, p)\n",
    "\n",
    "print(\"All threads have completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
